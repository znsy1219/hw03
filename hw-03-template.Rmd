---
title: "hw-03"
author: "Nanshuyuan Zhang(s2516281)"
date: "`r Sys.Date()`"
output: html_document
---



```{r setup, include=FALSE}
## **DO NOT EDIT THIS CODE CHUNK**
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(tidymodels)
```


## Data load and preparation before modelling

```{r read_data}
gss16<-read.csv("data/gss16.csv")
```

#### Cleaning and selecting columns

```{r}
gss16_advfront <- gss16 %>%
  select(advfront, emailhr, educ, polviews, wrkstat) %>%
  drop_na()
```

#### Re-levelling `advfront`

```{r}
gss16_advfront <- gss16_advfront %>%
  mutate(
    advfront = case_when(
      advfront == "Strongly agree" ~ "Agree",
      advfront == "Agree" ~ "Agree",
      TRUE ~ "Not agree"
    ),
    advfront = fct_relevel(advfront, "Not agree", "Agree")
  )
```

#### Re-levelling `polviews`

```{r}
gss16_advfront <- gss16_advfront %>%
  mutate(
    polviews = case_when(
      str_detect(polviews, "[Cc]onservative") ~ "Conservative",
      str_detect(polviews, "[Ll]iberal") ~ "Liberal",
      TRUE ~ polviews
    ),
    polviews = fct_relevel(polviews, "Conservative", "Moderate", "Liberal")
  )
```

#### Creating a new `fulltime` variable

```{r}
gss16_advfront <- gss16_advfront %>%
  mutate(fulltime = ifelse(wrkstat == "Working fulltime",TRUE,FALSE))
```


## Exercise 1: Create a linear regression model

#### Exercise 1 (a)

```{r}

model <- lm(emailhr ~ educ + fulltime, data = gss16_advfront)

summary(model)


glance(model)


```

*Your answer here*

#### Exercise 1 (b)

```{r}

print(glance(model))

par(mfrow = c(2, 2))  
plot(model)


ggplot(gss16_advfront, aes(x = educ, y = emailhr, color = fulltime)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE) +
  labs(title = "Relationship Between Education, Fulltime Status, and Email Hours",
       x = "Education (Years)", y = "Email Hours per Week",
       color = "Full-time Status") +
  theme_minimal()


```

*Your answer here*

## Exercise 2: Create a workflow to fit a model

```{r split-data}
set.seed(1234)
gss16_split <- initial_split(gss16_advfront)
gss16_train <- training(gss16_split)
gss16_test  <- testing(gss16_split)
```

#### Exercise 2 (a)

```{r}

gss16_rec_1 <- recipe(advfront ~ educ, data = gss16_train)

gss16_mod_1 <- logistic_reg() %>%
  set_engine("glm")

gss16_wflow_1 <- workflow() %>%
  add_recipe(gss16_rec_1) %>%
  add_model(gss16_mod_1)

```

The higher the number of years of education, the more likely a person is to support funding for scientific research.

#### Exercise 2 (b)

```{r}
# replace this with your code
```

The response variable advfront is binary. Logistic regression is a standard method for binary classification. Also it is a computationally efficient model.
Logistic regression fits the requirements of this task (binary classification), and is computationally efficient. While more complex models like decision trees or random forests could be used, they would add unnecessary complexity and reduce interpretability for this specific problem.


#### Exercise 2 (c)

```{r}
gss16_fit_1 <- fit(gss16_wflow_1, data = gss16_train)

tidy(gss16_fit_1)

```

Intercept: When educ = 0, the log-odds of agreeing with the statement are -0.3750807.
educ: For every additional year of education, the log-odds of agreeing increase by 0.1497668.
Significance: Both coefficients are statistically significant (p.value < 0.5).

Model Equation: log-odds(advfront="Agree")=-0.3750807+0.1497668â‹…educ

## Exercise 3: Logistic regression with single predictor

#### Exercise 3 (a)

```{r}
# replace this with your code
```

*Your answer here*

#### Exercise 3 (b)

```{r}
# replace this with your code
```

*Your answer here*

## Exercise 4: Logistic regression modelling and interpretation

#### Exercise 4 (a)

```{r}
# replace this with your code
```

*Your answer here*

#### Exercise 4 (b)
  
```{r}
# replace this with your code
```

*Your answer here*

#### Exercise 4 (c) 

```{r}
# replace this with your code
```

*Your answer here*

